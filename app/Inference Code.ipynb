{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00000-9ce32d15-c5a7-4894-963f-5e667f50494c",
        "deepnote_cell_type": "code"
      },
      "source": "%pylab inline\nimport pandas as pd\nimport librosa\nimport glob \nimport numpy as np \nfrom keras.models import model_from_json\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings; warnings.simplefilter('ignore')\nfrom scipy.special import softmax",
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Populating the interactive namespace from numpy and matplotlib\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00001-20b96f0a-230f-4a88-8080-3ec03e8689e0",
        "deepnote_cell_type": "code"
      },
      "source": "def inference(audio, audio_duration):\n    #audio should be a string\n    #sampel audio format: 'output10.wav'\n    \n    #audio_duration should be a real number \n    \n    #loading pre-trained model\n    json_file = open('model.json', 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    loaded_model = model_from_json(loaded_model_json)\n    loaded_model.load_weights(\"Emotion_Voice_Detection_Model.h5\")\n\n    #loading audio input\n    X, sample_rate = librosa.load(audio, res_type='kaiser_fast',duration=audio_duration,sr=22050*2,offset=0.5)\n    \n    #changing data shape\n    sample_rate = np.array(sample_rate)\n    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n    featurelive = mfccs\n    livedf2 = featurelive\n    \n    livedf2= pd.DataFrame(data=livedf2)\n    livedf2 = livedf2.stack().to_frame().T\n    \n    twodim= np.expand_dims(livedf2, axis=2)\n    \n    #inference\n    livepreds = loaded_model.predict(twodim, \n                         batch_size=32, \n                         verbose=1)\n    \n    #choosing top 3 values\n    agg_livepreds = []\n    for i in range(5):\n        agg_livepreds.append(livepreds[0][i]+livepreds[0][i+5])\n    \n    \n    idx = np.argsort(agg_livepreds)[-3:]\n    idx = np.flip(idx)\n    prob = softmax(agg_livepreds)\n    prob_list = []\n    for i in idx:\n        prob_list.append(prob[i]*100)\n    string_prob_list = [str(round(p, 3))+\"%\" for p in prob_list]\n    \n#     livepreds1=livepreds.argmax(axis=1)\n#     liveabc = livepreds1.astype(int).flatten()\n    \n#     pred = liveabc[0]\n    \n    emotion_dict = {\n    \n    0 : 'angry',\n    1 : 'calm',\n    2 : 'fearful',\n    3 : 'happy',\n    4 : 'sad',\n\n    }\n    \n    #emotion = emotion_dict.get(pred).split('_')[1]\n    #emotion = emotion_dict.get(pred)\n    emotion_list = [emotion_dict.get(i) for i in idx]\n    \n    \n    \n    #return 3 value pairs (emotion, probability)\n    #possible emotion values: 'angry', 'calm', 'fearful', 'happy', 'sad'\n    output = list(zip(emotion_list, string_prob_list))\n    \n    return output\n    \n    ",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00002-cdeda45d-c296-4c78-9fd0-ac9aed944545",
        "deepnote_cell_type": "code"
      },
      "source": "#using the demo audio\ninference('angry.wav', 2.5)",
      "execution_count": 65,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\r1/1 [==============================] - 0s 370ms/step\n"
        },
        {
          "data": {
            "text/plain": "[('angry', '40.460970997810364%'),\n ('sad', '14.884759485721588%'),\n ('happy', '14.884759485721588%')]"
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00003-189427b2-e49f-4426-871a-823aab95af83",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b1523f23-efe9-44d2-8414-3bde3bc8ab39' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "deepnote_notebook_id": "07f5835c-8396-4bf5-aac3-46781cd8a802",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}